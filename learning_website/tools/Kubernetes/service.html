<html><u><center><h2>Kubernetes Networking and Services</h2></center></u>
<h4><pre>
Kubernetes Networking addresses four concerns :-
1. Containers within pod use networking to communicate via loopback.
2. Cluster Networking provides communication between different pods.
3. The service resources let you expose an application running in the pods to be reachable from outside your cluster.
4. You can also use services to publish services only for consumption inside your cluster.
   
Lab 1: Container to container communication on same pod happens through localhost within the container.

kind: Pod
apiVersion: v1
metadata:
  name: testpod
spec:
  containers:
    - name: c00
      image: ubuntu
      command: ["/bin/bash", "-c", "while true; do echo Hello-Bhupinder; sleep 5 ; done"]
    - name: c01
      image: httpd
      ports:
       - containerPort: 80

Lab 2: Now try to establish communication between two different pods within same machine
-> Pod to pod communication on same worker node happens through pod IP
-> By default Pod's IP will be be accessible from outside the node. 

Lab steps

-------------

Problem: Each pod get its own IP address, however in a deployment, the set of pods running in one moment in time could be different from the set of pods
running that application a moment later.

This leads to a problem: If some set of pods(call them 'backends') provide functionality to other pods ('call them frontend') inside your cluster, how do
the frontends find out and keep track of which IP address to connect to, so that the frontend can use the backend from of the workload?  

-> When using RC, pods are terminated and created during scaling or replication operations.
-> When using deployments, while updating the image version the pods are terminated and new pods take the place of other pods.
-> Pods are very dynamic i.e. they come and go on the k8s cluster and on any of the available node & it would be diffcult to access the pods as the pods
IP changes once its recreated.
-> Service object is an logical bridge between pods and end users, which provide Virtual IP (VIP).
-> Service allows clients to reliably connect to the containers running in the pod using the VIP.
-> The VIP is not an actual IP connected to a network interface, but its purpose is purely to forward traffic to one or more pods.
-> Kube proxy is the one which keeps the mapping between the VIP and the pods upto date, which queries the API server to learn about new services in the cluster.
-> Although each pod has a unique IP address, those IP's are not exposed outside the cluster.
-> Services helps to expose the VIP mapped to the pods and allows application to receive traffic.
-> Labels are used to select which are the pods to be put under service.
-> Creating a service will create an endpoint to access the pods/application in it.
-> Services can be exposed in different ways by specifying a type in the service spec.
   1. ClusterIP
   2. NodePort
   3. LoadBalancer - Created by cloud providers that will route external traffic to every node on the NodePort. (eg. ELB on AWS)
   4. Headless - Creates several endpoints that are used to produce DNS records. Each record is bound to a pod
-> By default, Service can run only between ports 30,000-32767
-> The set of pods targeted by a service is usually determined by a selector.

ClusterIP

-> Exposes VIP only reachable from within the cluster.
-> Mainly used to communicate between components of microservices.    

kind: Deployment
apiVersion: apps/v1
metadata:
   name: mydeployments
spec:
   replicas: 1
   selector:      # tells the controller which pods to watch/belong to
    matchLabels:
     name: deployment
   template:
     metadata:
       name: testpod1
       labels:
         name: deployment
     spec:
      containers:
        - name: c00
          image: httpd
          ports:
          - containerPort: 80
====================
kind: Service                             # Defines to create Service type Object
apiVersion: v1
metadata:
  name: demoservice
spec:
  ports:
    - port: 80                               # Containers port exposed
      targetPort: 80                     # Pods port
  selector:
    name: deployment                    # Apply this service to any pods which has the specific label
  type: ClusterIP                       # Specifies the service type i.e ClusterIP or NodePort

$ kubectl get svc


NodePort

-> Make a service accessible from outside the cluster.
-> Exposes the service on the same port of each selected node in the cluster using NAT. 


</pre></h4>
</html>